---
title: "Introduction to R package fastfrechet"
output: rmarkdown::html_vignette
bibliography: '`r system.file("REFERENCES.bib", package="fastfrechet")`'
vignette: >
  %\VignetteIndexEntry{Introduction to R package fastfrechet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.align = "center", 
               out.width = "70%",
               fig.width = 6, fig.height = 5.5)
```

```{r setup}
library(fastfrechet)
```

# Introduction

`fastfrechet` is an R package providing fast and robust solutions to several computational problems associated with Fréchet regression [@petersen_frechet_2019] and an associated variable selection method [@tucker_variable_2023], focusing on a specific metric space setting. Fréchet regression generalizes regression with Euclidean covariates and Euclidean responses, to settings where responses exist in a general metric space (i.e. non-vector space). The space of univariate distribution response objects is one such example, recently gaining popularity in biomedical applications where technological advances have made rich patient-level data sets commonplace in a variety of settings. These larger data sets and complex response constraints require newer, faster algorithms for feasibility and scalability [@coulter_fast_2024], which `fastfrechet` provides.\

This vignette walks through an example use case of all of the available functions in `fastfrechet`. This includes simulating example covariate-dependent distribution responses, implementing Fréchet regression, performing variable selection, and utilizing resampling tools to supplement automatic variable selection. Where appropriate, the mathematical problems being solved by each function are briefly described. A second vignette, `monotoneQP-fastfrechet`, provides a more detailed algorithmic exposition of the function `monotoneQP` specifically.


# Running Through an Example

Since `fastfrechet` performs regression tasks on distribution responses, we illustrate its functionality with simulated distributions. In this section we generate covariate-dependent, zero-inflated negative binomial distributions. We then perform Fréchet regression, and the basic variable selection task with the Fréchet Ridge Selection Operator (FRiSO). Detailed mathematical treatment of these topics can be found in @petersen_frechet_2019, @tucker_variable_2023, and @coulter_fast_2024.

## Generating quantile functions

The zero-inflated negative binomial (`zinbinom`) distribution is a mixture distribution. It can be described as the marginal distribution of random variable $Y$ utilizing variable $\Psi \sim \mathrm{Bernoulli}(z)$ which turns "on" or "off" the zero inflation: $(Y|\Psi=1) \sim \delta_{\{0\}}$, and $(Y|\Psi=0) \sim \mathrm{nbinom}(r, p)$. As stated, there are three parameters to $Y$'s distribution: the zero-inflation parameter $z \in (0, 1)$, the size parameter of the negative binomial part $r > 0$, and the probability parameter of the negative binomial part $p > 0$. We represent the distribution of $Y$ with its quantile function, given by
$$
F_Y^{-1}(p) = F_{Y|\Psi=0}^{-1}\left( \frac{p-z}{1-z} \right),
$$
where $F_{Y|\Psi=0}^{-1}$ is the quantile function of $\mathrm{nbinom}(r, p)$ and $F_{Y|\Psi=0}^{-1}(p) = 0$ for $p < 0$.

### Using `fastfrechet::generate_zinbinom_qf`

The `generate_zinbinom_qf` function from `fastfrechet` generates covariate-dependent quantile functions from the `zinbinom` distribution, in line with the Experiment B setting from @coulter_fast_2024. The function takes as inputs

* `n`, the number of response-covariate pairs,
* `p`, the number of covariates, and
* `m`, the grid density to evaluate the quantile functions in the $(0, 1)$ interval.

`generate_zinbinom_qf` also optionally takes "baseline" parameters of the `zinbinom` distribution, namely, the zero-inflation factor, and the probability and size parameters of the `nbinom` component. As output, `generate_zinbinom_qf` returns a list containing the `n` $\times$ `p` covariate matrix `X`; and the `n` $\times$ `m` response matrix `Y`, which stores the quantile functions row-wise.

```{r}
n = 200  # number of samples - nrow(X) and nrow(Y).
p = 20   # number of covariates - ncol(X).
m = 200  # quantile function grid density - ncol(Y).
mseq = seq(1 / (2 * m), 1 - 1 / (2 * m), length.out = m)

set.seed(31)
mydata = fastfrechet::generate_zinbinom_qf(n = n,
                                           p = p,
                                           m = m)
 
X = mydata$X  # (n x p) matrix of covariates
Y = mydata$Y  # (n x m) matrix of quantile functions, stored row-wise
```


### Plotting

We illustrate these quantile function responses with line plots. Note the true functions are piece-wise constant, where the line plots interpolate between points on the observed `m`-grid.

```{r}
matplot(mseq, t(Y), xlim = c(0, 1), ylim = range(Y), type = 'l', lty = 1, col = 1,
        main = "znbinom Quantile Functions", xlab = "p", ylab = "quantile")
```


## Fréchet regression

Fréchet regression generalizes Euclidean regression to the general metric space setting. In the sample setting, we observe covariate-response pairs $\{(\mathbf{x}_i, \mathbf{y}_i)\} \subset \mathbb{R}^p \times \Omega$, where $\Omega$ is the space of univariate quantile functions equipped with the 2-Wasserstein metric $d_W(\mathbf{q}, \mathbf{p}) = \lVert\mathbf{q} - \mathbf{p}\rVert_{L_2[0,1]}$. (Generally we also only observe the distributions over a discrete grid, not as full functions.) The conditional Fréchet mean is estimated by
$$
\widehat{\mathbf{q}}_{\oplus}(\mathbf{x}_i) = \underset{\mathbf{q}\in\Omega}{\mathrm{argmin}} \frac{1}{n}\sum_{j=1}^n \left\{ 1 + \mathbf{x}_i^{\top}(\mathbf{X}^{\top}\mathbf{X})^+\mathbf{x}_j\right\} d_W^2(\mathbf{y}_j, \mathbf{q}).
$$
This is the Fréchet regression problem, a weighted Fréchet mean problem which can be solved by quadratic programming. This is implemented by `frechetreg_univar2wass`, using a dual active-set method inspired from @arnstrom_dual_2022.

### Using `fastfrechet::frechetreg_univar2wass`

The `frechetreg_univar2wass` function from `fastfrechet` takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix,
* `Z`, an optional "output" covariate matrix for which to evaluate the conditional Fréchet means, if not `X`,
* `C_init`, an optional warm-start estimate of the constraint active sets (see the `monotoneQP-fastfrechet` vignette for more details),
* `lambda`, an optional sparsity vector from the variable selection method (see the next section), and
* `lower` and `upper`, support constraints on the distributions.

Since the `zinbinom` distribution is bounded from below by zero, we set `lower = 0`, and use default settings for other parameters. As output, `frechetreg_univar2wass` returns a list containing the set of fitted quantile functions `Qhat`, stored row-wise as in `Y`; and the `n` $\times$ `m+1` matrix of Lagrange multipliers associated with the solution, also stored row-wise.

```{r}
lower = 0
upper = Inf
# Estimate conditional quantile functions:
output = fastfrechet::frechetreg_univar2wass(X = X,
                                             Y = Y,
                                             Z = NULL,
                                             C_init = NULL,
                                             lambda = NULL,
                                             lower = 0,
                                             upper = Inf)
 
# Note: to numerical precision, these quantile functions are non-decreasing...
min(apply(output$Qhat, 1, diff))
 
# ...and bounded from below by the lower bound, zero:
min(output$Qhat)
```


### Plotting $\widehat{\mathbf{Y}}$ and $\widehat{\mathbf{Q}}$

The Fréchet means associated with the Fréchet regression problem $\widehat{\mathbf{Q}}$ are projections of the unconstrained Euclidean response object $\widehat{\mathbf{Y}}$ onto the space of quantile functions. Row-wise, the matrix $\widehat{\mathbf{Y}}$ generally does not obey quantile function constraints of monotonicity or box constraints, whereas the solution $\widehat{\mathbf{Q}}$ does. We recover the unconstrained matrix `Yhat` from the output of `frechetreg_univar2wass`, and illustrate it side-by-side the fitted $\widehat{\mathbf{Q}}$.

```{r, fig.width = 8, fig.height = 5.5}
# Plot the unconstrained estimators and the constrained conditional quantile functions:
Yhat = output$Qhat - output$Lagrange_Multiplier[ , -(m + 1)] + output$Lagrange_Multiplier[ , -1]
par(mfrow = c(1, 2), las = 1)
matplot(mseq, t(Yhat), type = "l", lty = 1, col = 'black', xlab = "p", ylab = "Yhat",
        main = "Unconstrained Solution")
matplot(mseq, t(output$Qhat), type = "l", lty = 1, col = 'black', xlab = "p", ylab = "Qhat",
        main = "Fitted Quantile Functions")
```


## Variable selection with Fréchet ridge selection operator

`fastfrechet` implements the variable selection procedure of @tucker_variable_2023 through the function `FRiSO_univar2wass`. This penalization procedure, called the Fréchet Ridge Selection Operator (FRiSO), solves for a sparse vector $\pmb{\lambda}(\tau) \in \mathbb{R}^p$. Each entry of the optimal $\widehat{\pmb{\lambda}}(\tau)$ is either positive, which indicates the corresponding variable is selected in the final ($\tau$-dependent) model, or is zero, which indicates the corresponding variable is not selected. The entries of $\pmb{\lambda}(\tau)$ add up to $\tau$, so colloquially $\tau$ acts as a "total allowance", and $\widehat{\pmb{\lambda}}(\tau)$ is an optimal allocation of that allowance among the variables. Theoretical details of this procedure are given in @wu_cant_2021 and @tucker_variable_2023; details of the $\widehat{\pmb{\lambda}}(\tau)$ estimation method implemented by `fastfrechet` are given in @coulter_fast_2024.

### Using `fastfrechet::FRiSO_univar2wass`

The `FRiSO_univar2wass` function from `fastfrechet` takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix,
* `lower` and `upper`, support constraints on the distributions,
* `tauseq`, a vector of $\tau$ values for which to evaluate $\pmb{\lambda}(\tau)$, ideally provided in-sequence,
* `eps`, an error tolerance parameter, and
* `nudge`, a parameter to "push" the numeric solver away from non-optimal saddle points.

In this illustrative example, we evaluate the variable selection problem on a dense grid `tauseq = seq(0.2, 20, 0.2)`, and set `eps = 0.001` and `nudge = 0.01`. Other optional parameters allow the user to control the gradient descent algorithm, including dampening, step size, momentum, and maximum number of iterations; the help page has more details. `FRiSO_univar2wass` returns a matrix of fitted $\widehat{\pmb{\lambda}}$ values, stored column-wise, one column for each value in `tauseq`.

```{r}
# Dense grid of "allowance" totals:
tauseq = seq(0.2, 20, 0.2)

# Generate estimated "allowance vector"s \lambda for each \tau, stored
# column-wise in matrix `L`:
L = FRiSO_univar2wass(X = X,
                      Y = Y,
                      lower = 0,
                      upper = Inf,
                      tauseq = tauseq,
                      eps = 0.001,
                      nudge = 0.01)
```



### Plotting $\widehat{\pmb{\lambda}}(\tau)$ solution paths

As functions of $\tau$, the fitted $\widehat{\pmb{\lambda}}$ values trace out "solution paths" that we illustrate with line plots. Each line path corresponds to a covariate: if the line is above zero, that covariate is selected, and if it is equal to zero, that covariate is not selected. We illustrate the solution paths of the first four covariates, on which the simulated quantile functions depend, with red lines. The other covariates are illustrated with black lines.

```{r}                             
# Plot FRiSO "allowance vector" solution paths:
matplot(tauseq, t(L), type = "l", col = c(rep("red", 4), rep("black", p - 4)),
        lty = 1, xlab = expression(tau), ylab = expression(lambda*"("*tau*")"),
        main = "Variable Selection Solution Paths", las = 1, lwd = 1.5)
legend("topleft", lwd = 1.5, col = c("red", "black"), bty = "n",
       legend = c("Model variable", "Unimportant variable"))
```

# Resampling for Variable Selection

The variable selection procedure depends on a hyperparameter $\tau$ which is loosely related to the final model size. Two ways of choosing a final model free of $\tau$ are cross-validation (CV) and complementary pairs stability selection (CPSS). In CV, detailed in @tucker_variable_2023, final model selection occurs by finding $\tau_{\mathrm{opt}}$ which minimizes out-of-sample error, and selecting those variables indicated by $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$. In CPSS, detailed in @shah_variable_2013 and @coulter_fast_2024, variable selection stability is measured across repeated half-splits of the data, with the most consistently selected variables being chosen in a way to guarantee pointwise error control.

## Cross-validation

`fastfrechet` implements K-fold CV for the variable selection procedure through the function `FRiSO_CV_univar2wass`, with an option to perform leave-one-out CV. Out-of-sample testing is implemented with refitting, a procedure described and illustrated in more detail in @tucker_variable_2023.

### Using `fastfrechet::FRiSO_CV_univar2wass`

The `FRiSO_CV_univar2wass` function from `fastfrechet` takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix,
* `K`, number of "folds" for K-fold CV (default 10), and
* `thresh`, a threshold parameter, where $\widehat{\lambda}_k(\tau) >$ `thresh` means the $k^{\mathrm{th}}$ parameter is selected.

`FRiSO_CV_univar2wass` calls `FRiSO_univar2wass` internally, and additionally takes any parameter which can be fed into that function. This includes box constraint parameters `lower` and `upper`, the parameter for $\tau$ values `tauseq`, and optimization algorithm control parameters. The help page for `FRiSO_univar2wass` contains information on how all of these further parameters can be utilized. As output, `FRiSO_CV_univar2wass` returns a list containing

* `tauseq`, the sequence of $\tau$ values as provided to the function,
* `errors`, a matrix of per-fold CV errors across `tauseq`,
* `error_sum`, a vector of fold-aggregate CV errors across `tauseq`,
* `opt_tau`, the value in `tauseq` minimizing `error_sum`,
* `opt_lambda`, the fitted $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$ vector for the optimally chosen $\tau$, and
* `opt_selected`, the indices of the variables in the model corresponding to $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$.

In this example, we use 10-fold CV, and set a selection threshold of `thresh = 0.0001`.

```{r}
# Set cross-validation parameters
K = 10
thresh = 0.0001
eps = 0.001

# Run cross-validation
cv = FRiSO_CV_univar2wass(X = X,
                          Y = Y,
                          K = K,
                          thresh = thresh,
                          lower = lower,
                          upper = upper,
                          tauseq = tauseq,
                          eps = eps)
```

### Plotting, interpreting CV errors and optimal solution

We plot the CV errors in each fold, along with the average CV error across `tauseq`. This lets us see for which $\tau$ value the minimum CV error occurs. We also can print the indices of the variables selected by the model fit with this optimal $\tau$.

```{r}
# Plot errors per fold and average fold error:
matplot(tauseq, cv$errors, type = 'l', lty = 1, main = "Cross-Validation Fold Errors")
lines(tauseq, cv$error_sum / K, lwd = 3)
points(cv$opt_tau, min(cv$error_sum) / K, pch = 1, lwd = 2, cex = 1.5)
 
# Identify which variables are selected in "optimal" model:
cv$opt_selected
```

As we can see, 10-fold CV correctly identifies the first four variables, which are known ahead of time to be the only ones which influence the Fréchet means. However, in this example the final model selects more variables as well; it is larger than the true model.

## Complementary pairs stability selection

`fastfrechet` implements complementary pairs stability selection (CPSS) for the variable selection procedure through the function `FRiSO_CPSS_univar2wass`. @shah_variable_2013 demonstrate that CPSS allows pointwise (i.e. across $\tau$) control on "low selection probability" variables. This process for establishing a minimum selection threshold for error control is described and illustrated in more detail by @shah_variable_2013, and @coulter_fast_2024 as applied to Fréchet regression. `fastfrechet` calculates these CPSS selection thresholds through the function `Shah_Samworth_thresholds`.

### Using `fastfrechet::FRiSO_CPSS_univar2wass`

The `FRiSO_CPSS_univar2wass` function from `fastfrechet` is used analogously as `FRiSO_CV_univar2wass`. It takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix `Y`,
* `B`, the number of data half-splits to perform (default 50), and
* `thresh`, the same selection threshold parameter as in `FRiSO_CV_univar2wass`.

`FRiSO_CPSS_univar2wass` also calls `FRiSO_univar2wass` internally, and additionally takes any parameter which can be fed into that function. As output, `FRiSO_CV_univar2wass` returns a list containing

* `tauseq`, the sequence of $\tau$ values as provided to the function,
* `selected_variables`, a `B` $\times$ `2` $\times$ `length(tauseq)` $\times$ `p` array indicating which variables were selected in the $2B$ split halves, across `tauseq`,
* `selected_samples`, a `B` $\times$ `2` $\times$ `length(tauseq)` $\times$ `n` array indicating how samples were sorted in the $2B$ split halves,
* `stability_paths`, a `length(tauseq)` $\times$ `p` matrix containing the empirical selection proportions of all variables across `tauseq`, and
* `model_size_est`. a `length(tauseq)`-long vector containing the average model size, i.e. number of selected parameters, across `tauseq`.

In this example, we use `B = 50` half-splits, and evaluate CPSS on a coarser $\tau$ grid, `tauseq = 1:10`.

```{r}

# Set complementary pairs stability selection parameters
B = 50
thresh = 0.0001
tauseq = 1:10
eps = 0.001
 
# Run complementary pairs stability selection
cpss = FRiSO_CPSS_univar2wass(X = X,
                              Y = Y,
                              B = B,
                              thresh = thresh,
                              lower = lower,
                              upper = upper,
                              tauseq = tauseq,
                              eps = eps)

```

### Plotting, interpreting stability paths

We plot the CPSS "stability path" for each variable, which is the empirical proportion of split halves ($2B=100$ total) the variable is selected by FRiSO. This lets us see which variables are most frequently selected.

```{r}
# Plot stability paths
matplot(cpss$tau, cpss$stability_paths, type = 'l', lty = 1, lwd = 2,
        col = c(rep('red', 4), rep('black', p - 4)),
        ylab = "Stability Paths",
        xlab = bquote(tau))
```

As we can see, the four model variables are all selected with high probability across $\tau$. However, as $\tau$ increases, the model size tends to increase. In practice, it is advisable to perform CPSS up to a limit on the `p`-relative model size.

### Shah and Samworth error control

The function `Shah_Samworth_thresholds` calculates the error control thresholds from @shah_variable_2013 by using the model size estimates from the CPSS output. The control is on the number of final variables selected which have "low selection probability" (LSP). `Shah_Samworth_thresholds` uses the recommendation from @shah_variable_2013 to define "low" as below the average relative model size. As input, `Shah_Samworth_thresholds` takes

* `p`, the number of covariates in the full model,
* `q`, the average model size (possibly estimated) after variable selection,
* `B`, the number of data half-splits in the CPSS procedure, and
* `E_thr`, the limit on the expected number of selected LSP variables.

As output, `Shah_Samworth_thresholds` returns a list containing

* `E_thr`, the LSP selection limit as provided to the function,
* `B`, the number of data half-splits as provided to the function,
* `relative_model_size`, the estimated $\theta = q/p$ relative model sizes based on inputs `p` and `q`, and
* `pointwise_thresholds`, the calculated selection thresholds for each $q$ which pointwise control expected LSP variables.

As the threshold is a function of model size, and hence a function of $\tau$, a final variable set could be selected from those variables whose "stability paths" exceed the "threshold path". We set the error control at `E_thr = 1`, that is, pointwise the expected number of LSP variables is bounded above by 1.

```{r}
# Calculate thresholds using Shah and Samworth (2013) method:
shahsam = Shah_Samworth_thresholds(p = p,
                                   q = cpss$model_size_est,
                                   B = B,
                                   E_thr = 1)

# Redraw the line plot, but add the threshold path:
matplot(cpss$tau, cpss$stability_paths, type = 'l', lty = 1, lwd = 2,
        col = c(rep('red', 4), rep('black', p - 4)),
        ylab = "Stability Paths",
        xlab = bquote(tau))
lines(cpss$tau, shahsam$pointwise_thresholds, type = "o", pch = 16)
```

The "stability path" of each of the four true model variables exceeds the "threshold path" at some point. If we select variables on that basis, the final model contains each of the true variables, and no incorrect variable. For reference, the range of relative model sizes over `tauseq` is approximately 0.15 to 0.45.
```{r}
range(cpss$model_size_est / p)
```