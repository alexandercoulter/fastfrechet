---
title: "Introduction to R package fastfrechet"
output: rmarkdown::html_vignette
bibliography: '`r system.file("REFERENCES.bib", package="fastfrechet")`'
vignette: >
  %\VignetteIndexEntry{Introduction to R package fastfrechet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-options, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(
  fig.align = "center",
  out.width = "70%",
  fig.width = 6, fig.height = 5.5
)
```

```{r setup}
library(fastfrechet)
```

# Introduction

`fastfrechet` is an R package providing fast and robust solutions to several
computational problems associated with Fréchet regression
[@petersen_frechet_2019] and an associated variable selection method
[@tucker_variable_2023], focusing on a specific metric space setting. Fréchet
regression generalizes regression with Euclidean covariates and Euclidean
responses, to settings where responses exist in a general metric space (i.e.
non-vector space). The space of univariate distribution response objects is one
such example, recently gaining popularity in biomedical applications where
technological advances have made rich patient-level data sets commonplace in a
variety of settings. These larger data sets and complex response constraints
require newer, faster algorithms for feasibility and scalability
[@coulter_fast_2024], which `fastfrechet` provides.\

This vignette walks through example use cases of all of the available functions
in `fastfrechet`. This includes simulating example covariate-dependent
distribution responses, implementing Fréchet regression, performing variable
selection, and utilizing resampling tools to supplement automatic variable
selection. Where appropriate, the mathematical problems being solved by each
function are briefly described. A second vignette, `monotoneQP-fastfrechet`,
provides a more detailed algorithmic exposition of the function `monotoneQP`
specifically.


# Running Through Examples

Since `fastfrechet` performs regression tasks on distribution responses, we
illustrate its functionality with simulated distributions. In this section we
generate covariate-dependent, zero-inflated negative binomial distributions. We
then perform Fréchet regression, and the basic variable selection task with the
Fréchet Ridge Selection Operator (FRiSO). Detailed mathematical treatment of
these topics can be found in @petersen_frechet_2019, @tucker_variable_2023, and
@coulter_fast_2024.

## Generating quantile functions

The zero-inflated negative binomial (`zinbinom`) distribution is a mixture
distribution. It can be described as the marginal distribution of random
variable $Y$ utilizing variable $\Psi \sim \mathrm{Bernoulli}(z)$ which turns
"on" or "off" the zero inflation: $(Y|\Psi=1) \sim \delta_{\{0\}}$, and
$(Y|\Psi=0) \sim \mathrm{nbinom}(r, p)$. As stated, there are three parameters
to $Y$'s distribution: the zero-inflation parameter $z \in (0, 1)$, the size
parameter of the negative binomial part $r > 0$, and the probability parameter
of the negative binomial part $p > 0$. We represent the distribution of $Y$ with
its quantile function, given by
$$
F_Y^{-1}(u) = F_{Y|\Psi=0}^{-1}\left( \frac{u-z}{1-z} \right),
$$
where $F_{Y|\Psi=0}^{-1}$ is the quantile function of $\mathrm{nbinom}(r, p)$
and $F_{Y|\Psi=0}^{-1}(u) = 0$ for $u < 0$.

### Using `fastfrechet::generate_zinbinom_qf`

The `generate_zinbinom_qf` function from `fastfrechet` generates
covariate-dependent quantile functions from the `zinbinom` distribution, in line
with the Experiment B setting from @coulter_fast_2024. The function takes as
inputs

* `n`, the number of response-covariate pairs,
* `p`, the number of covariates, and
* `m`, the grid density to evaluate the quantile functions in the $(0, 1)$
interval.

`generate_zinbinom_qf` also optionally takes "baseline" parameters of the
`zinbinom` distribution, namely, the zero-inflation factor, and the probability
and size parameters of the `nbinom` component. As output, `generate_zinbinom_qf`
returns a list containing the `n` $\times$ `p` covariate matrix `X`; and the
`n` $\times$ `m` response matrix `Y`, which stores the quantile functions
row-wise.

```{r generate-data}
n <- 200 # number of samples - nrow(X) and nrow(Y).
p <- 20 # number of covariates - ncol(X).
m <- 200 # quantile function grid density - ncol(Y).
mseq <- seq(1 / (2 * m), 1 - 1 / (2 * m), length.out = m)

set.seed(31)
mydata <- fastfrechet::generate_zinbinom_qf(
  n = n,
  p = p,
  m = m
)

X <- mydata$X # (n x p) matrix of covariates
Y <- mydata$Y # (n x m) matrix of quantile functions, stored row-wise
```


### Plotting

We illustrate these quantile function responses with line plots. Note the true
functions are piece-wise constant, where the line plots interpolate between
points on the observed `m`-grid.

```{r plot-EQFs}
matplot(mseq, t(Y),
  xlim = c(0, 1), ylim = range(Y), type = "l", lty = 1, col = 1,
  main = "znbinom Quantile Functions", xlab = "u", ylab = "quantile"
)
```


## Fréchet regression

Fréchet regression generalizes Euclidean regression to the general metric space
setting. In the sample setting, we observe covariate-response pairs
$\{(\mathbf{x}_i, \mathbf{y}_i)\} \subset \mathbb{R}^p \times \Omega$, where
$\Omega$ is the space of univariate quantile functions equipped with the
2-Wasserstein metric
$d_W(\mathbf{q}, \mathbf{p}) = \lVert\mathbf{q} - \mathbf{p}\rVert_{L_2[0,1]}$.
(Generally we also only observe the distributions over a discrete grid, not as
full functions.) We treat the covariate vectors $\mathbf{x}_i$ as rows of matrix
$\mathbf{X}$, which is column-centered and scaled so that
$\mathrm{diag}(\mathbf{X}^{\top}\mathbf{X}) = \pmb{1}$. The conditional Fréchet
mean at $X = \mathbf{x}_*$ is estimated by
$$
\widehat{\mathbf{q}}_{\oplus}(\mathbf{x}_*) := \underset{\mathbf{q}\in\Omega}{\mathrm{argmin}} \sum_{i=1}^n \left\{ \frac{1}{n} + \mathbf{x}_*^{\top}(\mathbf{X}^{\top}\mathbf{X})^+\mathbf{x}_i\right\} d_W^2(\mathbf{y}_i, \mathbf{q}).
$$
This is the Fréchet regression problem, a weighted Fréchet mean problem solvable
by quadratic programming. This is implemented by `frechetreg_univar2wass`, using
a dual active-set method inspired from @arnstrom_dual_2022.

### Using `fastfrechet::frechetreg_univar2wass`

The `frechetreg_univar2wass` function from `fastfrechet` takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix,
* `Z`, an optional "output" covariate matrix for which to evaluate the
conditional Fréchet means, if not `X`,
* `C_init`, an optional warm-start estimate of the constraint active sets (see
the `monotoneQP-fastfrechet` vignette for more details),
* `lambda`, an optional sparsity vector from the variable selection method (see
the next section), and
* `lower` and `upper`, support constraints on the distributions.

Since the `zinbinom` distribution is bounded from below by zero, we set
`lower = 0`, and use default settings for other parameters. As output,
`frechetreg_univar2wass` returns a list containing the set of fitted quantile
functions `Qhat`, stored row-wise as in `Y`; and the `n` $\times$ `m+1` matrix
of Lagrange multipliers associated with the solution, also stored row-wise.

```{r run-frechetreg}
lower <- 0
upper <- Inf
# Estimate conditional quantile functions:
output <- fastfrechet::frechetreg_univar2wass(
  X = X,
  Y = Y,
  Z = NULL,
  C_init = NULL,
  lambda = NULL,
  lower = 0,
  upper = Inf
)

# Note: to numerical precision, these quantile functions are non-decreasing...
min(apply(output$Qhat, 1, diff))

# ...and bounded from below by the lower bound, zero:
min(output$Qhat)
```


### Plotting $\widehat{\mathbf{Y}}$ and $\widehat{\mathbf{Q}}$

The Fréchet means associated with the Fréchet regression problem
$\widehat{\mathbf{Q}}$ are projections of the unconstrained Euclidean response
object $\widehat{\mathbf{Y}}$ onto the space of quantile functions. Row-wise,
the matrix $\widehat{\mathbf{Y}}$ generally does not obey quantile function
constraints of monotonicity or box constraints, whereas the solution
$\widehat{\mathbf{Q}}$ does. We recover the unconstrained matrix `Yhat` from the
output of `frechetreg_univar2wass`, and illustrate it side-by-side the fitted
$\widehat{\mathbf{Q}}$.

```{r plot-unconstrained-constrained, fig.width = 8, fig.height = 5.5}
# Plot the unconstrained estimators and the constrained conditional quantile functions:
Yhat <- output$Qhat - output$Lagrange_Multiplier[, -(m + 1)] + output$Lagrange_Multiplier[, -1]
par(mfrow = c(1, 2), las = 1)
matplot(mseq, t(Yhat),
  type = "l", lty = 1, col = "black", xlab = "p", ylab = "Yhat",
  main = "Unconstrained Solution"
)
matplot(mseq, t(output$Qhat),
  type = "l", lty = 1, col = "black", xlab = "p", ylab = "Qhat",
  main = "Fitted Quantile Functions"
)
```

### An Example with $\widehat{\mathbf{Z}}$

To illustrate how the "output" covariate matrix `Z` works—i.e. predicting
distribution responses on non-training data—we specify an example matrix which
is zero everywhere, except for the first column. The first covariate, which is
known to be associated with the distribution responses, ranges over an
increasing sequence of values.

```{r frechetreg-with-Z}
# Create a Z matrix
nz <- 20
Z <- matrix(data = 0, nrow = nz, ncol = p)
Z[, 1] <- seq(from = -5, to = 5, length.out = nz)

# Run frechetreg_univar2wass
outputZ <- fastfrechet::frechetreg_univar2wass(
  X = X,
  Y = Y,
  Z = Z,
  C_init = NULL,
  lambda = NULL,
  lower = 0,
  upper = Inf
)

# Note: to numerical precision, these quantile functions are non-decreasing...
min(apply(outputZ$Qhat, 1, diff))

# ...and bounded from below by the lower bound, zero:
min(outputZ$Qhat)
```
The plot of unconstrained estimators and fitted quantile functions shows how
increasing the first covariate leads to changes in the distribution responses.

```{r plot-Z, fig.width = 8, fig.height = 5.5}
# Plot the unconstrained estimators and the constrained conditional quantile functions:
Yhat <- outputZ$Qhat - outputZ$Lagrange_Multiplier[, -(m + 1)] + outputZ$Lagrange_Multiplier[, -1]
par(mfrow = c(1, 2), las = 1)
matplot(mseq, t(Yhat),
  type = "l", lty = 1, col = hcl.colors(nz, palette = "Spectral"), xlab = "p", ylab = "Yhat",
  main = "Unconstrained Solution"
)
matplot(mseq, t(outputZ$Qhat),
  type = "l", lty = 1, col = hcl.colors(nz, palette = "Spectral"), xlab = "p", ylab = "Qhat",
  main = "Fitted Quantile Functions"
)
```

In the example above, we knew the first covariate is associated with the
distributional responses. Below, we change the values of a covariate known to
not associate with the distributional response; correspondingly, the fitted
distributions are more "constant".

```{r frechetreg-with-Z-2, fig.width = 8, fig.height = 5.5}
# Create a Z matrix
nz <- 20
Z <- matrix(data = 0, nrow = nz, ncol = p)
Z[, 5] <- seq(from = -5, to = 5, length.out = nz)

# Run frechetreg_univar2wass
outputZ <- fastfrechet::frechetreg_univar2wass(
  X = X,
  Y = Y,
  Z = Z,
  C_init = NULL,
  lambda = NULL,
  lower = 0,
  upper = Inf
)

# Plot the unconstrained estimators and the constrained conditional quantile functions:
Yhat <- outputZ$Qhat - outputZ$Lagrange_Multiplier[, -(m + 1)] + outputZ$Lagrange_Multiplier[, -1]
par(mfrow = c(1, 2), las = 1)
matplot(mseq, t(Yhat),
  type = "l", lty = 1, col = hcl.colors(nz, palette = "Spectral"), xlab = "p", ylab = "Yhat",
  main = "Unconstrained Solution"
)
matplot(mseq, t(outputZ$Qhat),
  type = "l", lty = 1, col = hcl.colors(nz, palette = "Spectral"), xlab = "p", ylab = "Qhat",
  main = "Fitted Quantile Functions"
)
```





## Low-Rank Settings

When $\mathbf{X}$ is not full column rank, and optional parameter `lambda` is
not specified, there are restrictions on what form the output covariate matrix
$\mathbf{X}$ may take.  Specifically, given the internal centering and scaling
performed on $\mathbf{X}$ and $\mathbf{Z}$ in order to insert an isolated
intercept column [@petersen_frechet_2019], so that in effect the Fréchet
regression is performed on matrices
$(\begin{matrix} \pmb{1} & \mathbf{X} \end{matrix})$ and
$(\begin{matrix} \pmb{1} & \mathbf{Z} \end{matrix})$, the requirement for the
Fréchet regression solution on $\mathbf{Z}$ to be unique is that each row in
$(\begin{matrix} \pmb{1} & \mathbf{Z} \end{matrix})$ has to belong to the
row-space of $(\begin{matrix} \pmb{1} & \mathbf{X} \end{matrix})$. When
$\mathbf{X}$ is full column rank, this condition always holds; when it is not,
this condition holds if each row of $\mathbf{Z}$ is an *affine combination* of
the rows of $\mathbf{X}$.

We can illustrate what happens when we remove rows from the example data set to
make it low-rank.
```{r low-rank-X}
output <- frechetreg_univar2wass(X = X[1:5, ], Y = Y[1:5, ], Z = NULL, lower = 0)
```
```{r low-rank-X-graph, echo = FALSE}
matplot(t(output$Qhat),
  type = "l", lty = 1, col = "black", las = 1,
  xlab = "u", ylab = "", main = "Low Rank X : n = 5, p = 10"
)
mtext(expression(widehat(Q)(u)), 2, 2.25)
```
Arbitrary `Z` inputs can throw an error in the low rank setting.
```{r low-rank-X-Z-arbitrary}
Z <- matrix(rnorm(10 * p), 10, p)
output <- tryCatch(
  expr = frechetreg_univar2wass(X = X[1:5, ], Y = Y[1:5, ], Z = Z, lower = 0),
  error = function(e) {
    return(e)
  }
)
output$message
```
On the other hand, if $\mathbf{X}$ is not full column rank, but there exists a
matrix $\mathbf{A} \in \mathbb{R}^{z \times n}$ such that
$\mathbf{Z} = \mathbf{A}\mathbf{X}$, and each row $\mathbf{A}$ sums to 1, then
$\mathbf{Z}$ obeys the requisite row-space constraint to obtain a unique
Fréchet regression solution.
```{r low-rank-X-Z-affine}
A <- matrix(rnorm(10 * 5), 10, 5)
A <- A / rowSums(A)
Z <- A %*% X[1:5, ]
output <- tryCatch(
  expr = frechetreg_univar2wass(X = X[1:5, ], Y = Y[1:5, ], Z = Z, lower = 0),
  error = function(e) {
    return(e)
  }
)
output$message # This is NULL, reflecting there is no error message
```
The `NULL` "message" indicates no error was encountered.  Arbitrary linear
combinations will generally not suffice, however.
```{r low-rank-X-Z-linear}
A <- matrix(rnorm(10 * 5), 10, 5)
# A <- A / rowSums(A)
Z <- A %*% X[1:5, ]
output <- tryCatch(
  expr = frechetreg_univar2wass(X = X[1:5, ], Y = Y[1:5, ], Z = Z, lower = 0),
  error = function(e) {
    return(e)
  }
)
output$message
```


## Variable selection with Fréchet ridge selection operator

`fastfrechet` implements the variable selection procedure of @tucker_variable_2023 through the function `FRiSO_univar2wass`. This penalization procedure, called the Fréchet Ridge Selection Operator (FRiSO), solves for a sparse vector $\widehat{\pmb{\lambda}}(\tau) \in \mathbb{R}^p$, defined via
$$
\begin{align}
\widehat{\pmb{\lambda}}(\tau) &:= \underset{\pmb{\lambda} \in \mathbb{R}^p}{\mathrm{argmin}} \sum_{i=1}^n d_W^2\left(\widehat{\mathbf{q}}(\mathbf{x}_i, \pmb{\lambda}), \: \mathbf{y}_i \right), \qquad \pmb{\lambda} \geq \pmb{0}, \quad \pmb{1}^{\top}\pmb{\lambda} = \tau, \\
\widehat{\mathbf{q}}(\mathbf{x}_i,\pmb{\lambda}) &:= \underset{\mathbf{q}\in\Omega}{\mathrm{argmin}} \sum_{j=1}^n \left( \frac{1}{n} + \mathbf{x}_i^{\top}(\mathbf{X}^{\top}\mathbf{X} + \mathbf{D}_{\pmb{\lambda}}^{-1})^{-1}\mathbf{x}_j \right) d_W^2(\mathbf{q}, \: \mathbf{y}_j).
\end{align}
$$
Each entry of the optimal $\widehat{\pmb{\lambda}}(\tau)$ is either positive, which indicates the corresponding variable is selected in the final ($\tau$-dependent) model, or is zero, which indicates the corresponding variable is not selected. The entries of $\pmb{\lambda}(\tau)$ add up to $\tau$, so colloquially $\tau$ acts as a "total allowance", and $\widehat{\pmb{\lambda}}(\tau)$ is an optimal allocation of that allowance among the variables. Theoretical details of this variable selection procedure are given in @wu_cant_2021 and @tucker_variable_2023. Details of the $\widehat{\pmb{\lambda}}(\tau)$ estimation method implemented by `fastfrechet` are given in @coulter_fast_2024.

### Using `fastfrechet::FRiSO_univar2wass`

The `FRiSO_univar2wass` function from `fastfrechet` takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix,
* `lower` and `upper`, support constraints on the distributions,
* `tauseq`, a vector of $\tau$ values for which to evaluate $\pmb{\lambda}(\tau)$, ideally provided in-sequence,
* `eps`, an error tolerance parameter, and
* `nudge`, a parameter to "push" the numeric solver away from non-optimal saddle points.

In this illustrative example, we evaluate the variable selection problem on a dense grid `tauseq = seq(0.2, 20, 0.2)`, and set `eps = 0.001` and `nudge = 0.01`. Other optional parameters allow the user to control the gradient descent algorithm, including dampening, step size, momentum, and maximum number of iterations; the help page has more details. `FRiSO_univar2wass` returns a matrix of fitted $\widehat{\pmb{\lambda}}$ values, stored column-wise, one column for each value in `tauseq`.

```{r run-FRiSO, eval = FALSE}
# Dense grid of "allowance" totals:
tauseq <- seq(0.2, 20, 0.2)

# Generate estimated "allowance vector"s \lambda for each \tau, stored
# column-wise in matrix `L`:
L <- FRiSO_univar2wass(
  X = X,
  Y = Y,
  lower = 0,
  upper = Inf,
  tauseq = tauseq,
  eps = 0.001,
  nudge = 0.01
)
```

```{r load-FRiSO, include = FALSE}
tauseq <- seq(0.2, 20, 0.2)
s <- system.file("private", "L_intro.Rda", package = "fastfrechet")
if (file.exists(s)) load(s)
```



### Plotting $\widehat{\pmb{\lambda}}(\tau)$ solution paths

As functions of $\tau$, the fitted $\widehat{\pmb{\lambda}}$ values trace out "solution paths" that we illustrate with line plots. Each line path corresponds to a covariate: if the line is above zero, that covariate is selected, and if it is equal to zero, that covariate is not selected. We illustrate the solution paths of the first four covariates, on which the simulated quantile functions depend, with red lines. The other covariates are illustrated with black lines.

```{r plot-friso}                             
# Plot FRiSO "allowance vector" solution paths:
matplot(tauseq, t(L),
  type = "l", col = c(rep("red", 4), rep("black", p - 4)),
  lty = 1, xlab = expression(tau), ylab = expression(lambda * "(" * tau * ")"),
  main = "Variable Selection Solution Paths", las = 1, lwd = 1.5
)
legend("topleft",
  lwd = 1.5, col = c("red", "black"), bty = "n",
  legend = c("Model variable", "Unimportant variable")
)
```
# Resampling for Variable Selection

The variable selection procedure depends on a hyperparameter $\tau$ which is loosely related to the final model size. Two ways of choosing a final model free of $\tau$ are cross-validation (CV) and complementary pairs stability selection (CPSS). In CV, detailed in @tucker_variable_2023, final model selection occurs by finding $\tau_{\mathrm{opt}}$ which minimizes out-of-sample error, and selecting those variables indicated by $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$. In CPSS, detailed in @shah_variable_2013 and @coulter_fast_2024, variable selection stability is measured across repeated half-splits of the data, with the most consistently selected variables being chosen in a way to guarantee pointwise error control.

## Cross-validation

`fastfrechet` implements K-fold CV for the variable selection procedure through the function `FRiSO_CV_univar2wass`, with an option to perform leave-one-out CV. Out-of-sample testing is implemented with refitting, a procedure described and illustrated in more detail in @tucker_variable_2023.

### Using `fastfrechet::FRiSO_CV_univar2wass`

The `FRiSO_CV_univar2wass` function from `fastfrechet` takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix,
* `K`, number of "folds" for K-fold CV (default 10), and
* `thresh`, a threshold parameter, where $\widehat{\lambda}_k(\tau) >$ `thresh` means the $k^{\mathrm{th}}$ parameter is selected.

`FRiSO_CV_univar2wass` calls `FRiSO_univar2wass` internally, and additionally takes any parameter which can be fed into that function. This includes box constraint parameters `lower` and `upper`, the parameter for $\tau$ values `tauseq`, and optimization algorithm control parameters. The help page for `FRiSO_univar2wass` contains information on how all of these further parameters can be utilized. As output, `FRiSO_CV_univar2wass` returns a list containing

* `tauseq`, the sequence of $\tau$ values as provided to the function,
* `errors`, a matrix of per-fold CV errors across `tauseq`,
* `error_sum`, a vector of fold-aggregate CV errors across `tauseq`,
* `opt_tau`, the value in `tauseq` minimizing `error_sum`,
* `opt_lambda`, the fitted $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$ vector for the optimally chosen $\tau$, and
* `opt_selected`, the indices of the variables in the model corresponding to $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$.

In this example, we use 10-fold CV, and set a selection threshold of `thresh = 0.0001`.

```{r run-cv, eval = FALSE}
# Set cross-validation parameters
K <- 10
thresh <- 0.0001
eps <- 0.001

# Run cross-validation
cv <- FRiSO_CV_univar2wass(
  X = X,
  Y = Y,
  K = K,
  thresh = thresh,
  lower = lower,
  upper = upper,
  tauseq = tauseq,
  eps = eps
)
```

```{r load-cv, include = FALSE}
K <- 10
thresh <- 0.0001
eps <- 0.001
s <- system.file("private", "cv_intro.Rda", package = "fastfrechet")
if (file.exists(s)) load(s)
```

### Plotting, interpreting CV errors and optimal solution

We plot the CV errors in each fold, along with the average CV error across `tauseq`. This lets us see for which $\tau$ value the minimum CV error occurs. We also can print the indices of the variables selected by the model fit with this optimal $\tau$.

```{r plot-cv}
# Plot errors per fold and average fold error:
matplot(tauseq, cv$errors, type = "l", lty = 1, main = "Cross-Validation Fold Errors")
lines(tauseq, cv$error_sum / K, lwd = 3)
points(cv$opt_tau, min(cv$error_sum) / K, pch = 1, lwd = 2, cex = 1.5)

# Identify which variables are selected in "optimal" model:
cv$opt_selected
```

As we can see, 10-fold CV correctly identifies the first three variables, however it fails to correctly identify the fourth variable.

## Complementary pairs stability selection

`fastfrechet` implements complementary pairs stability selection (CPSS) for the variable selection procedure through the function `FRiSO_CPSS_univar2wass`. @shah_variable_2013 demonstrate that CPSS allows pointwise (i.e. across $\tau$) control on "low selection probability" (LSP) variables. This process for establishing a minimum selection threshold for error control is described and illustrated in more detail by @shah_variable_2013, and @coulter_fast_2024 as applied to Fréchet regression. `fastfrechet` calculates these CPSS selection thresholds through the function `Shah_Samworth_thresholds`.

### Using `fastfrechet::FRiSO_CPSS_univar2wass`

The `FRiSO_CPSS_univar2wass` function from `fastfrechet` is used analogously as `FRiSO_CV_univar2wass`. It takes as input

* `X`, the covariate matrix,
* `Y`, the quantile function response matrix `Y`,
* `B`, the number of data half-splits to perform (default 50), and
* `thresh`, the same selection threshold parameter as in `FRiSO_CV_univar2wass`.

`FRiSO_CPSS_univar2wass` also calls `FRiSO_univar2wass` internally, and additionally takes any parameter which can be fed into that function. As output, `FRiSO_CPSS_univar2wass` returns a list containing

* `tauseq`, the sequence of $\tau$ values as provided to the function,
* `selected_variables`, a `B` $\times$ `2` $\times$ `length(tauseq)` $\times$ `p` array indicating which variables were selected in the $2B$ split halves, across `tauseq`,
* `selected_samples`, a `B` $\times$ `2` $\times$ `length(tauseq)` $\times$ `n` array indicating how samples were sorted in the $2B$ split halves,
* `stability_paths`, a `length(tauseq)` $\times$ `p` matrix containing the empirical selection proportions of all variables across `tauseq`, and
* `model_size_est`. a `length(tauseq)`-long vector containing the average model size, i.e. number of selected parameters, across `tauseq`.

In this example, we use `B = 50`  data half-splits, and evaluate CPSS on a coarser $\tau$ grid, `tauseq = 1:10`.

```{r run-cpss, eval = FALSE}
# Set complementary pairs stability selection parameters
B <- 50
thresh <- 0.0001
tauseq <- 1:10
eps <- 0.001

# Run complementary pairs stability selection
cpss <- FRiSO_CPSS_univar2wass(
  X = X,
  Y = Y,
  B = B,
  thresh = thresh,
  lower = lower,
  upper = upper,
  tauseq = tauseq,
  eps = eps
)
```

```{r load-cpss, include = FALSE}
B <- 50
thresh <- 0.0001
tauseq <- 1:10
eps <- 0.001
s <- system.file("private", "cpss_intro.Rda", package = "fastfrechet")
if (file.exists(s)) load(s)
```

### Plotting, interpreting stability paths

We plot the CPSS "stability path" for each variable, which is the empirical proportion of split halves ($2B=100$ total) the variable is selected by FRiSO. This lets us see which variables are most frequently selected.

```{r plot-cpss}
# Plot stability paths:
matplot(cpss$tau, cpss$stability_paths,
  type = "l", lty = 1, lwd = 2,
  col = c(rep("red", 4), rep("black", p - 4)),
  ylab = "Selection Proportions", xlab = bquote(tau),
  main = "CPSS Stability Paths"
)
```

As we can see, the four model variables are all selected with high probability across $\tau$. However, as $\tau$ increases, the model size tends to increase. In practice, it is advisable to perform CPSS up to a limit on the `p`-relative model size.

### Shah and Samworth error control

The function `Shah_Samworth_thresholds` calculates the error control thresholds from @shah_variable_2013 by using the model size estimates from the CPSS output. The control is on the number of final selected LSP variables. `Shah_Samworth_thresholds` uses the recommendation from @shah_variable_2013 to define "low" as below the average relative model size. As input, `Shah_Samworth_thresholds` takes

* `p`, the number of covariates in the full model,
* `q`, the average model size (possibly estimated) after variable selection,
* `B`, the number of data half-splits in the CPSS procedure, and
* `E_thr`, the limit on the expected number of selected LSP variables.

As output, `Shah_Samworth_thresholds` returns a list containing

* `E_thr`, the LSP selection limit as provided to the function,
* `B`, the number of data half-splits as provided to the function,
* `relative_model_size`, the estimated $\theta = q/p$ relative model sizes based on inputs `p` and `q`, and
* `pointwise_thresholds`, the calculated selection thresholds for each $q$ which pointwise control expected LSP variables.

As the threshold is a function of model size, and hence a function of $\tau$, a final variable set could be selected from those variables whose "stability paths" exceed the "threshold path". We set the error control at `E_thr = 1`, that is, pointwise the expected number of LSP variables is bounded above by 1.

```{r run-shah_samworth}
# Calculate thresholds using Shah and Samworth (2013) method:
shahsam <- Shah_Samworth_thresholds(
  p = p,
  q = cpss$model_size_est,
  B = B,
  E_thr = 1
)

# Redraw the line plot, but add the threshold path:
matplot(cpss$tau, cpss$stability_paths,
  type = "l", lty = 1, lwd = 2,
  col = c(rep("red", 4), rep("black", p - 4)),
  ylab = "Selection Proportions", xlab = bquote(tau),
  main = "CPSS Stability Paths with Selection Threshold"
)
lines(cpss$tau, shahsam$pointwise_thresholds, type = "o", pch = 16)
```

The "stability path" of each of the four true model variables exceeds the "threshold path" at some point. If we select variables on that basis, the final model contains each of the true variables, and no incorrect variable. For reference, the range of relative model sizes over `tauseq` is approximately 0.15 to 0.45.
```{r show-model-size}
range(cpss$model_size_est / p)
```
