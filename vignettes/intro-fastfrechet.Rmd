---
title: "Introduction to R package fastfrechet"
output: rmarkdown::html_vignette
bibliography: '`r system.file("REFERENCES.bib", package="fastfrechet")`'
vignette: >
  %\VignetteIndexEntry{Introduction to R package fastfrechet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.align = "center", 
               out.width = "70%",
               fig.width = 6, fig.height = 5.5)
```

```{r setup}
library(fastfrechet)
```

# Introduction

`fastfrechet` is an R package providing fast and robust solutions to several computational problems associated with Fréchet regression [@petersen_frechet_2019] and an associated variable selection method [@tucker_variable_2023], focusing on a specific metric space setting. Fréchet regression generalizes regression with Euclidean covariates and Euclidean responses, to settings where responses exist in a general metric space (i.e. non-vector space). The space of univariate distribution response objects is one such example, recently gaining popularity in biomedical applications where technological advances have made rich patient-level data sets commonplace in a variety of settings. These larger data sets and complex response constraints require newer, faster algorithms for feasibility and scalability [@coulter_fast_2024], which `fastfrechet` provides.\

This vignette walks through an example use case of all of the available functions in `fastfrechet`. This includes simulating example covariate-dependent distribution responses, implementing Fréchet regression, performing variable selection, and utilizing resampling tools to supplement automatic variable selection. Where appropriate, the mathematical problems being solved by each function are briefly described. A second vignette, `monotoneQP-fastfrechet`, provides a more detailed algorithmic exposition of the function `monotoneQP` specifically.


# Running Through an Example

Since `fastfrechet` performs regression tasks on distribution responses, we illustrate its functionality with simulated distributions. In this section we generate covariate-dependent, zero-inflated negative binomial distributions. We then perform Fréchet regression, and the basic variable selection task with the Fréchet Ridge Selection Operator (FRiSO). Detailed mathematical treatment of these topics can be found in @petersen_frechet_2019, @tucker_variable_2023, and @coulter_fast_2024.

## Generating quantile functions

The zero-inflated negative binomial (`zinbinom`) distribution is a mixture distribution. It can be described as the marginal distribution of random variable $Y$ utilizing variable $\Psi \sim \mathrm{Bernoulli}(z)$ which turns "on" or "off" the zero inflation: $(Y|\Psi=1) \sim \delta_{\{0\}}$, and $(Y|\Psi=0) \sim \mathrm{nbinom}(r, p)$. As stated, there are three parameters to $Y$'s distribution: the zero-inflation parameter $z \in (0, 1)$, the size parameter of the negative binomial part $r > 0$, and the probability parameter of the negative binomial part $p > 0$. We represent the distribution of $Y$ with its quantile function, given by
$$
F_Y^{-1}(p) = F_{Y|\Psi=0}^{-1}\left( \frac{p-z}{1-z} \right),
$$
where $F_{Y|\Psi=0}^{-1}$ is the quantile function of $\mathrm{nbinom}(r, p)$ and $F_{Y|\Psi=0}^{-1}(p) = 0$ for $p < 0$.

### Using `fastfrechet::generate_zinbinom_qf`

The `generate_zinbinom_qf` function from `fastfrechet` generates covariate-dependent quantile functions from the `zinbinom` distribution, in line with the Experiment B setting from @coulter_fast_2024. We specify the desired number of response-covariate pairs (given by `n`), the desired number of covariates (`p`), and the desired grid density to evaluate the quantile functions in the $(0, 1)$ interval (`m`). As output, we are provided the covariate matrix `X` (an `n`$\times$`p` matrix), and the response matrix `Y` (an `n`$\times$`m` matrix) which stores the quantile functions row-wise.

```{r}
n = 200  # number of samples - nrow(X) and nrow(Y).
p = 20   # number of covariates - ncol(X).
m = 200  # quantile function grid density - ncol(Y).
mseq = seq(1 / (2 * m), 1 - 1 / (2 * m), length.out = m)

set.seed(31)
mydata = fastfrechet::generate_zinbinom_qf(n = n,
                                           p = p,
                                           m = m)
 
X = mydata$X  # (n x p) matrix of covariates
Y = mydata$Y  # (n x m) matrix of quantile functions, stored row-wise
```


### Plotting

We illustrate these quantile function responses with line plots. Note the true functions are piecewise constant, where the line plots interpolate between points on the observed `m`-grid.

```{r}
matplot(mseq, t(Y), xlim = c(0, 1), ylim = range(Y), type = 'l', lty = 1, col = 1,
        main = "znbinom Quantile Functions", xlab = "p", ylab = "quantile")
```


## Fréchet regression

Fréchet regression generalizes Euclidean regression to the general metric space setting. In the sample setting where we observe covariate-response pairs $\{(\mathbf{x}_i, \mathbf{y}_i)\} \subset \mathbb{R}^p \times \Omega$, and $\Omega$ is the space of univariate quantile functions equipped with the 2-Wasserstein metric $d_W(\mathbf{q}, \mathbf{p}) = \lVert\mathbf{q} - \mathbf{p}\rVert_{L_2[0,1]}$, the conditional Fréchet mean is estimated by
$$
\widehat{\mathbf{q}}_{\oplus}(\mathbf{x}_i) = \underset{\mathbf{q}\in\Omega}{\mathrm{\arg\:min}} \frac{1}{n}\sum_{j=1}^n \left\{ 1 + \mathbf{x}_i^{\top}(\mathbf{X}^{\top}\mathbf{X})^+\mathbf{x}_j\right\} d_W^2(\mathbf{y}_j, \mathbf{q}).
$$
This is the Fréchet regression problem, a weighted Fréchet mean problem which can be solved by quadratic programming. This is implemented by `frechetreg_univar2wass`, using a dual active-set method inspired from @arnstrom_dual_2022.

### Using `fastfrechet::frechetreg_univar2wass`

The `frechetreg_univar2wass` function from `fastfrechet` takes as input the covariate matrix `X` and the quantile function response matrix `Y`, along with object `Z` (an optional "output" covariate matrix for which to evaluate the conditional Fréchet means, if not `X`), `C_init` (an optional warm-start estimate of the constraint active sets; see the `monotoneQP-fastfrechet` vignette for more details), `lambda` (an optional sparsity vector from the variable selection method; see the next section), and box constraints `lower` and `upper`.  Since the `zinbinom` distribution is bounded from below by zero, we set `lower = 0`, and use default settings for other parameters. The output list contains the set of fitted quantile functions `Qhat`, stored row-wise as in `Y`, and the matrix of Lagrange multipliers associated with the solution, also stored row-wise and of dimension `n`$\times$`m+1`.

```{r}
lower = 0
upper = Inf
# Estimate conditional quantile functions:
output = fastfrechet::frechetreg_univar2wass(X = X,
                                             Y = Y,
                                             Z = NULL,
                                             C_init = NULL,
                                             lambda = NULL,
                                             lower = 0,
                                             upper = Inf)
 
# Note: to numerical precision, these quantile functions are non-decreasing...
min(apply(output$Qhat, 1, diff))
 
# ...and bounded from below by the lower bound, zero:
min(output$Qhat)
```


### Plotting $\widehat{\mathbf{Y}}$ and $\widehat{\mathbf{Q}}$

The Fréchet means associated with the Fréchet regression problem $\widehat{\mathbf{Q}}$ are projections of the unconstrained Euclidean response object $\widehat{\mathbf{Y}}$ onto the space of quantile functions. Row-wise, the matrix $\widehat{\mathbf{Y}}$ generally does not obey quantile function constraints of monotonicity or box constraints, whereas the solution $\widehat{\mathbf{Q}}$ does. We recover the unconstrained matrix `Yhat` from the output of `frechetreg_univar2wass`, and illustrate it beside the fitted response matrix with line plots.

```{r, fig.width = 8, fig.height = 5.5}
# Plot the unconstrained estimators and the constrained conditional quantile functions:
Yhat = output$Qhat - output$Lagrange_Multiplier[ , -(m + 1)] + output$Lagrange_Multiplier[ , -1]
par(mfrow = c(1, 2), las = 1)
matplot(mseq, t(Yhat), type = "l", lty = 1, col = 'black', xlab = "p", ylab = "Yhat",
        main = "Unconstrained Solution")
matplot(mseq, t(output$Qhat), type = "l", lty = 1, col = 'black', xlab = "p", ylab = "Qhat",
        main = "Fitted Quantile Functions")
```


## Variable selection with Fréchet ridge selection operator

`fastfrechet` implements the variable selection procedure of @tucker_variable_2023 through the function `FRiSO_univar2wass`. This penalization procedure, called the Fréchet Ridge Selection Operator (FRiSO), solves for a sparse vector $\pmb{\lambda}(\tau) \in \mathbb{R}^p$. Each entry of the optimal $\widehat{\pmb{\lambda}}(\tau)$ is either positive, which indicates the corresponding covariate is selected in the final ($\tau$-dependent) model, or it is zero, which indicates the corresponding covariate is not selected. The entries of $\pmb{\lambda}(\tau)$ add up to $\tau$, so colloquially $\tau$ acts as a "total allowance", and $\widehat{\pmb{\lambda}}(\tau)$ is an optimal allocation of that allowance among the covariates. Theoretical details of this procedure are given in @wu_cant_2021 and @tucker_variable_2023; details of the $\widehat{\pmb{\lambda}}(\tau)$ estimation method implemented by `fastfrechet` are given in @coulter_fast_2024.

### Using `fastfrechet::FRiSO_univar2wass`

The `frechetreg_univar2wass` function from `fastfrechet` takes as input the covariate matrix `X` and the quantile function response matrix `Y`, along with `lower` and `upper` (box constraints for the support), `tauseq` (a vector of $\tau$ values for which to evaluate $\pmb{\lambda}(\tau)$, ideally provided in-sequence), `eps` (an error tolerance parameter), and `nudge` (a parameter to "push" the numeric solver away from non-optimal saddle points). In this illustrative example, we evaluate the variable selection problem on a dense grid `tauseq = seq(0.2, 20, 0.2)`, and set `eps = 0.001` and `nudge = 0.01`. The function returns a matrix of fitted $\widehat{\pmb{\lambda}}$ values, stored column-wise, one column for each value in `tauseq`.

```{r}
# Dense grid of "allowance" totals:
tauseq = seq(0.2, 20, 0.2)

# Generate estimated "allowance vector"s \lambda for each \tau, stored
# column-wise in matrix `L`:
L = FRiSO_univar2wass(X = X,
                      Y = Y,
                      lower = 0,
                      upper = Inf,
                      tauseq = tauseq,
                      eps = 0.001,
                      nudge = 0.01)
```



### Plotting $\widehat{\pmb{\lambda}}(\tau)$ solution paths

As functions of $\tau$, the fitted $\widehat{\pmb{\lambda}}$ values trace out "solution paths" that we illustrate with line plots. Each line path corresponds to a covariate: if the line is above zero, that covariate is selected, and if it is equal to zero, that covariate is not selected. We illustrate the solution paths of the first four covariates, on which the simulated quantile functions depend, with red lines. The other covariates are illustrated with black lines.

```{r}                             
# Plot FRiSO "allowance vector" solution paths:
matplot(tauseq, t(L), type = "l", col = c(rep("red", 4), rep("black", p - 4)),
        lty = 1, xlab = expression(tau), ylab = expression(lambda*"("*tau*")"),
        main = "Variable Selection Solution Paths", las = 1, lwd = 1.5)
legend("topleft", lwd = 1.5, col = c("red", "black"), bty = "n",
       legend = c("Model variable", "Unimportant variable"))
```

# Resampling for Variable Selection

The variable selection procedure depends on a hyperparameter $\tau$ which is loosely related to the final model size. Choosing a final model free of this choice of $\tau$ can be accomplished in a number of ways. Two ways are cross-validation and complementary pairs stability selection. In cross-validation, detailed in @tucker_variable_2023, final model selection occurs by choosing the $\tau_{\mathrm{opt}}$ which minimizes out-of-sample error, and selecting those variables indicated by $\widehat{\pmb{\lambda}}(\tau_{\mathrm{opt}})$. In complementary pairs stability selection, detailed in @shah_variable_2013 and @coulter_fast_2024, variable selection stability is measured across repeated data splits, with the most consistently selected variables being chosen in a way to guarantee pointwise error control over $\tau$.

## Cross-validation

`fastfrechet` implements K-fold cross-validation for the variable selection procedure through the function `FRiSO_CV_univar2wass`, with an option to perform leave-one-out cross-validation. Cross-validation testing is implemented with refitting, that is, the model is trained using the weighting scheme from the variable selection procedure, but the test responses are fit using the unweighted covariates. This process is described and illustrated in more detail by @tucker_variable_2023.

### Using `fastfrechet::FRiSO_CV_univar2wass`



```{r}
# Set cross-validation parameters
K = 10
thresh = 0.0001
eps = 0.001

# Run cross-validation
cv = FRiSO_CV_univar2wass(X = X,
                          Y = Y,
                          K = K,
                          thresh = thresh,
                          lower = lower,
                          upper = upper,
                          tauseq = tauseq,
                          eps = eps)
```

### Plotting, interpreting CV errors and optimal solution

```{r}
# Plot errors per fold and average fold error:
matplot(tauseq, cv$errors, type = 'l', lty = 1, main = "Cross Validation Fold Errors")
lines(tauseq, cv$error_sum / K, lwd = 3)
points(cv$opt_tau, min(cv$error_sum) / K, pch = 1, lwd = 2, cex = 1.5)
 
# Identify which variables are selected in "optimal" model:
cv$opt_selected
```

## Complementary pairs stability selection

### Using `fastfrechet::FRiSO_CPSS_univar2wass`

```{r}

# Set complementary pairs stability selection parameters
B = 50
thresh = 0.0001
tauseq = 1:10
eps = 0.001
 
# Run complementary pairs stability selection
cpss = FRiSO_CPSS_univar2wass(X = X,
                              Y = Y,
                              B = B,
                              thresh = thresh,
                              lower = lower,
                              upper = upper,
                              tauseq = tauseq,
                              eps = eps)

```

### Plotting, interpreting stability paths

```{r}
# Plot stability paths
matplot(cpss$tau, cpss$stability_paths, type = 'l', lty = 1, lwd = 2,
        col = c(rep('red', 4), rep('black', p - 4)),
        ylab = "Stability Paths",
        xlab = bquote(tau))
```

### Shah and Samworth error control



