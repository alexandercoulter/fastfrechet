---
title: "Performance Comparison Example to Accompany Manuscript"
output: rmarkdown::html_vignette
bibliography: '`r system.file("REFERENCES.bib", package="fastfrechet")`'
vignette: >
  %\VignetteIndexEntry{Performance Comparison Example to Accompany Manuscript}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

```{r setup}
library(fastfrechet)
library(WRI)
library(frechet)
library(microbenchmark)
```

```{r, include = FALSE}
s = system.file("private_code", "GlobalObj4h.R", package = "fastfrechet")
if(file.exists(s)) source(s)
s = system.file("private_code", "GloWassReg.R", package = "fastfrechet")
if(file.exists(s)) source(s)
s = system.file("private_code", "IndivRidgeGloWassReg.R", package = "fastfrechet")
if(file.exists(s)) source(s)
s = system.file("private_code", "lambdaGlobalcoordesc.R", package = "fastfrechet")
if(file.exists(s)) source(s)
```

# Purpose

The purpose of this vignette is to provide step-by-step instructions for
replicating the performance comparisons in the manuscript introducing this
package. We demonstrate superior computational speed and accuracy when compared
to existing Fréchet regression algorithms (for 2-Wasserstein space) from the
`WRI` package and the `frechet` package, and compared to existing variable
selection functions provided in the supplementary material of
@tucker_variable_2023. (For a general illustration of what Fréchet regression
and variable selection look like in 2-Wasserstein space, see the accompanying
`intro-fastfrechet` vignette.) These latter functions are not available in
package format; we assume the user has downloaded and sourced them into their
active `R` session for use.

The covariate-response objects we utilize for our comparisons are
simulated covariate-dependent zero-inflated negative binomial distributions. The
`fastfrechet` function `generate_zinbinom_qf` allows us to simulate $n$ such
distributions, represented as quantile functions evaluated on a shared $m$-grid
in $(0, 1)$, dependent on the first 4 of $p \geq 4$ covariates. The example data
set we generate for this vignette has sample size $n = 100$, covariate number
$p = 10$, and quantile functions evaluated on an equidistant $m = 100$ grid in
$(0, 1)$.
```{r}
set.seed(31)
n = 100        # number of quantile functions
p = 10         # number of covariates
m = 100        # (0, 1) quantile functions grid density 

# Equidistant m-grid in (0, 1)
mseq = seq(0.5 / m, 1 - 0.5 / m, length.out = m)

gendata = generate_zinbinom_qf(n, p, m)
X = gendata$X  # (n x p) covariate matrix
Y = gendata$Y  # (n x m) quantile response matrix, stored row-wise
```

```{r, include = FALSE}
png("zinb_examples.png", width = 6.5, height = 4, units = "in", res = 576)

par(mai = c(0.55, 0.6, 0.08, 0.08))
plot(x = c(), y = c(), xlim = c(-0.01, 1.01), ylim = c(0, max(Y)), axes = FALSE,
     xlab = "", ylab = "")
polygon(x = par("usr")[c(1, 2, 2, 1)], y = par("usr")[c(3, 3, 4, 4)],
        col = "gray93", border = NA)
abline(h = axTicks(2), v = axTicks(1), col = "white")
matlines(mseq, t(Y), col = "black", lty = 1, lwd = 1.5)
axis(1, NULL, TRUE, FALSE, -1.1, cex.axis = 0.9)
axis(2, NULL, TRUE, FALSE, -0.9, las = 1, cex.axis = 0.9)
mtext("p", 1, 1.4, cex = 1.2)
mtext("Q(p)", 2, 1.5, cex = 1.2)

dev.off()
```

![](zinb_examples.png){width=100%}

# The Fréchet Regression Problem

The (global) Fréchet regression problem in univariate 2-Wasserstein space is a
weighted Fréchet mean problem, where inputs and outputs are discretized
distribution functions. In this context, the most convenient distribution
representation is the quantile function, which must be monotone non-decreasing
and obey support constraints, when those are known. The R package `fastfrechet`
implements a fast and numerically exact custom solver for Fréchet regression in
2-Wasserstein space, inspired by the dual active-set method of
@arnstrom_dual_2022 (see the accompanying `monotoneQP-fastfrechet` vignette).
The function, `frechetreg_univar2wass`, permits user-specified box constraints,
and is natively amenable to the weighting scheme used in the variable selection
procedure described in the next section. It also solves the regression problem
in the case the covariate matrix $\pmb{X}$ is low-rank.

Fréchet regression implementations for 2-Wasserstein space are also available in
the R packages `WRI` (the `wass_regress` function) and `frechet` (the
`GloDenReg` function), as well as in the Supplementary Material of
@tucker_variable_2023 (the `GloWassReg` function). The implementation from the
`WRI` package can fit solutions in the low-rank setting, however it does not
take user-specified support constraints as input. It also requires the empirical
distribution objects be strictly monotone increasing, which excludes the
discretely supported distributions in our example; to include `WRI` in our
comparisons, we add a small monotone adjustment to the simulated distributions.
The implementations from `frechet` and the Supplementary Material from
@tucker_variable_2023 allow non-increasing distribution inputs and
user-specified box constraints. However, neither method can solve the regression
problem in the low-rank setting; the former is relatively slow, even for
small $m$; and the latter is not currently available in an R package.

## Computation Time Comparison

We compare the four methods on time to solve the Fréchet regression problem for
the example data set using the R package `microbenchmark`, iterating the
solving procedure 5 times for each method. We can see the implementation from
`fastfrechet` is the fastest of the four methods, up to $1000\times$ faster than
methods currently available in R packages.

```{r}
# WRI takes a data frame input:
X_df = as.data.frame(X)

# Add small monotone adjustment to Y for WRI:
Y_adj = Y + matrix(seq(0, 0.01, len = m), n, m, byrow = TRUE)

# Benchmark times over 5 iterations:
M = microbenchmark("WRI" = wass_regress( ~ ., X_df, "quantile", Y_adj),
                   "frechet" = GloDenReg(X, qin = Y, optns = list(lower = 0)),
                   "Tucker" = GloWassReg(X, Y, X, lower = 0),
                   "fastfrechet" = frechetreg_univar2wass(X, Y, lower = 0),
                   times = 5,
                   unit = "ms")

# Print times - milliseconds - rounding to 2 decimal places:
cbind(summary(M)[ , 1], round(summary(M)[ , -1], 2))
```

## Optimization Accuracy Comparison

We compare the four methods on optimization accuracy for the Fréchet regression
on the example data set. We can see the implementation from `WRI` does not obey
known box constraints because the option to specify them is not available; the
implementation from `frechet` violates the lower box constraint despite its
explicit inclusion as an argument. The implementation from the Supplementary
Material of @tucker_variable_2023 and the implementation from `fastfrechet` both
solve the problem to numerical accuracy.
```{r}
# Fit quantile functions:
WRI_Q = wass_regress( ~ ., X_df, "quantile", Y_adj)$Qfit
frechet_Q = GloDenReg(X, qin = Y, optns = list(lower = 0))$qout
Tucker_Q = GloWassReg(X, Y, X, lower = 0)
fastfrechet_Q = frechetreg_univar2wass(X, Y, lower = 0)$Qhat
```

```{r, include = FALSE}
# Plot quantile functions, focusing around zero:
# 0.4 + 4a + 4(0.06) = 6.5
a = (6.5 - 0.32 - 0.45) / 4
mais = cbind(c(0.35, 0.35, 0.35, 0.35),
             c(0.45, 0.04, 0.04, 0.04),
             c(0.2, 0.2, 0.2, 0.2),
             c(0.04, 0.04, 0.04, 0.08))
figs = cbind(c(0, 0.4 + a + 0.04, 0.4 + 2 * (a + 0.04), 0.4 + 3 * (a + 0.04)) / 6.5,
             c(0.4 + a + 0.04, 0.4 + 2 * (a + 0.04), 0.4 + 3 * (a + 0.04), 6.5) / 6.5,
             c(0, 0, 0, 0),
             c(1, 1, 1, 1))

png("frechetreg_accuracy_comparison.png", width = 6.5, height = 2.2, units = "in", res = 576)

par(fig = figs[1, ], mai = mais[1, ])
plot(x = c(), y = c(), xlim = c(-0.01, 1.01), ylim = c(-0.4, 0.4), axes = FALSE,
     xlab = "", ylab = "")
polygon(x = par("usr")[c(1, 2, 2, 1)], y = par("usr")[c(3, 3, 4, 4)],
        col = "gray93", border = NA)
abline(h = axTicks(2), v = axTicks(1), col = "white")
abline(h = 0, lty = 2, col = "gray50")
matlines(mseq, t(WRI_Q), col = "black", lty = 1, lwd = 1.5)
axis(1, axTicks(1), c("0", "0.2", "0.4", "0.6", "0.8", 1), FALSE, -1.3, cex.axis = 0.55)
axis(2, NULL, TRUE, FALSE, -0.9, las = 1, cex.axis = 0.55)
mtext("p", 1, 0.7, cex = 0.8)
mtext("Q(p)", 2, 1.25, cex = 0.8)
text(par("usr")[1] - 0.06, par("usr")[4] + 0.045,
     "WRI", pos = 4, xpd = TRUE, cex = 0.75, font = 2)
text(par("usr")[2] + 0.06, par("usr")[4] + 0.045,
     sprintf("%.4f sec", median(M$time[M$expr == "WRI"]) / 1e9),
     pos = 2, xpd = TRUE, cex = 0.6, font = 2)


par(fig = figs[2, ], mai = mais[2, ], new = TRUE)
plot(x = c(), y = c(), xlim = c(-0.01, 1.01), ylim = c(-0.4, 0.4), axes = FALSE,
     xlab = "", ylab = "")
polygon(x = par("usr")[c(1, 2, 2, 1)], y = par("usr")[c(3, 3, 4, 4)],
        col = "gray93", border = NA)
abline(h = axTicks(2), v = axTicks(1), col = "white")
abline(h = 0, lty = 2, col = "gray50")
matlines(mseq, t(frechet_Q), col = "black", lty = 1, lwd = 1.5)
axis(1, axTicks(1), c("0", "0.2", "0.4", "0.6", "0.8", 1), FALSE, -1.3, cex.axis = 0.55)
mtext("p", 1, 0.7, cex = 0.8)
text(par("usr")[1] - 0.06, par("usr")[4] + 0.045,
     "frechet", pos = 4, xpd = TRUE, cex = 0.75, font = 2)
text(par("usr")[2] + 0.06, par("usr")[4] + 0.045,
     sprintf("%.4f sec", median(M$time[M$expr == "frechet"]) / 1e9),
     pos = 2, xpd = TRUE, cex = 0.6, font = 2)


par(fig = figs[3, ], mai = mais[3, ], new = TRUE)
plot(x = c(), y = c(), xlim = c(-0.01, 1.01), ylim = c(-0.4, 0.4), axes = FALSE,
     xlab = "", ylab = "")
polygon(x = par("usr")[c(1, 2, 2, 1)], y = par("usr")[c(3, 3, 4, 4)],
        col = "gray93", border = NA)
abline(h = axTicks(2), v = axTicks(1), col = "white")
abline(h = 0, lty = 2, col = "gray50")
matlines(mseq, t(Tucker_Q), col = "black", lty = 1, lwd = 1.5)
axis(1, axTicks(1), c("0", "0.2", "0.4", "0.6", "0.8", 1), FALSE, -1.3, cex.axis = 0.55)
mtext("p", 1, 0.7, cex = 0.8)
text(par("usr")[1] - 0.06, par("usr")[4] + 0.045,
     "Tucker", pos = 4, xpd = TRUE, cex = 0.75, font = 2)
text(par("usr")[2] + 0.06, par("usr")[4] + 0.045,
     sprintf("%.4f sec", median(M$time[M$expr == "Tucker"]) / 1e9),
     pos = 2, xpd = TRUE, cex = 0.6, font = 2)


par(fig = figs[4, ], mai = mais[4, ], new = TRUE)
plot(x = c(), y = c(), xlim = c(-0.01, 1.01), ylim = c(-0.4, 0.4), axes = FALSE,
     xlab = "", ylab = "")
polygon(x = par("usr")[c(1, 2, 2, 1)], y = par("usr")[c(3, 3, 4, 4)],
        col = "gray93", border = NA)
abline(h = axTicks(2), v = axTicks(1), col = "white")
abline(h = 0, lty = 2, col = "gray50")
matlines(mseq, t(fastfrechet_Q), col = "black", lty = 1, lwd = 1.5)
axis(1, axTicks(1), c("0", "0.2", "0.4", "0.6", "0.8", 1), FALSE, -1.3, cex.axis = 0.55)
mtext("p", 1, 0.7, cex = 0.8)
text(par("usr")[1] - 0.06, par("usr")[4] + 0.045,
     "fastfrechet", pos = 4, xpd = TRUE, cex = 0.75, font = 2)
text(par("usr")[2] + 0.06, par("usr")[4] + 0.045,
     sprintf("%.4f sec", median(M$time[M$expr == "fastfrechet"]) / 1e9),
     pos = 2, xpd = TRUE, cex = 0.6, font = 2)

dev.off()
```

![](frechetreg_accuracy_comparison.png){width=100%}

# The Variable Selection Problem

The variable selection problem for Fréchet regression comprises finding an
optimal weight vector $\widehat{\pmb{\lambda}} \in \mathbb{R}^p$ that satisfies
a $\tau$-simplex constraint, for fixed hyperparameter $\tau > 0$. Solving the
variable selection problem involves iteratively solving a
$\pmb{\lambda}$-weighted version of the associated Fréchet regression problem.
In the 2-Wasserstein space setting, the optimal $\widehat{\pmb{\lambda}}$
essentially minimizes an $L^2$ norm between these weighted Fréchet means and the
raw data `Y`. (See the accompanying vignette `intro-fastfrechet`, and
@coulter_fast_2024 for a more detailed exposition.)

The R package `fastfrechet` implements variable selection for Fréchet regression
in 2-Wasserstein space, implementing the second-order geodesic descent algorithm
developed by @coulter_fast_2024. The function, `FRiSO_univar2wass`, includes two
new modifications. First, the implementation utilizes the custom dual active-set
method discussed in the previous subsection to solve the iterative
$\pmb{\lambda}$-weighted Fréchet regression problem. For each step, as weight
vector $\pmb{\lambda}^t \rightarrow \pmb{\lambda}^{t+1}$ is updated, the
previously estimated Lagrangian for the Fréchet regression solution is used as a
warm start for the subsequent iterate. Second, the implementation includes an
option for the user to specify an impulse parameter, to utilize momentum-based
geodesic descent.

Existing implementation of variable selection for Fréchet regression is
available in the Supplementary Material of @tucker_variable_2023 (the function
`IndivRidgeGloWassReg`, which solves the $\pmb{\lambda}$-weighted Fréchet
regression problem, and the function `lambdaGlobalcoordesc`, which is the
variable selection workhorse). The method is a particular implementation of an
algorithm which can be generally applied to Fréchet regression in other settings
besides 2-Wasserstein space. However, it is slow and does not natively handle
the low-rank setting, and is not currently available in an R package.

## Computation Time Comparison

We compare the two methods to solve the variable selection problem over a range
of hyperparameter values $\tau \in \{0.5, 1.0, \cdots, 10.0\}$, iterating the
solving procedure 5 times for each method. Since the implementation available
from @tucker_variable_2023 does not natively take multiple $\tau$ values as
input, we wrote a short wrapper function (given below) which loops over the
$\tau$ values, and each step uses the previously fitted
$\widehat{\pmb{\lambda}}$ vector as a warm start for the subsequent $\tau$
iterate.

```{r}
# Wrapper function for Tucker et al. (2023) code:
FRiSO_Tucker = function(X, Y, lower, upper, tauseq){
  
  p = ncol(X)
  Lambda = matrix(NA, nrow = p, ncol = length(tauseq))
  L = rep(tauseq[1] / p, p)
  for(t in 1:length(tauseq)){
    
    L = lambdaGlobalcoordesc(X, Y, tauseq[t], L, lower, upper)$lambda
    Lambda[ , t] = L
    
  }
  Lambda
  
}
```

We standardize the comparison to `fastfrechet` in two ways. First, as
the implementation from `fastfrechet` centers and scales $\pmb{X}$ to remove the
effect of unit choice on variable selection, we similarly center and scale
$\pmb{X}$ prior to using the implementation from @tucker_variable_2023. Second,
as the two algorithms are forms of iterative descent methods, comparison on
computation time should be made for exit conditions that attain solutions of
approximately equal quality. To this end, we use the method from
@tucker_variable_2023 "as is", and hand select `fastfrechet` error tolerance
of $\varepsilon = 0.014$. This gives `fastfrechet` solutions that minimize the
objective function over the $\tau$ range approximately as well as the solutions
from the other method. For completeness, we also run the `fastfrechet` method on
more stringent error tolerances, progressively halving the error tolerance
parameter.

The implementation from `fastfrechet` is upward of 20,000$\times$ faster than
the implementation from @tucker_variable_2023, to attain a solution of
approximately equal quality. Decreasing the `fastfrechet` error tolerance
parameter slightly increases computation time, but still maintains the
comparative speed benefits.

```{r}
# Centering and scaling X
X0 = (scale(X) * sqrt(n / (n - 1)))[,]

# Defining tau range, and error tolerances:
tauseq = seq(0.5, 10, by = 0.5)
epsseq = 0.014 / c(1, 2, 4, 8)

# Benchmark times over 5 iterations:
M = microbenchmark("Tucker" = FRiSO_Tucker(X0, Y, 0, 1e6, tauseq),
                   "fastfrechet_e" = FRiSO_univar2wass(X, Y, 0, Inf, tauseq, eps = epsseq[1]),
                   "fastfrechet_e/2" = FRiSO_univar2wass(X, Y, 0, Inf, tauseq, eps = epsseq[2]),
                   "fastfrechet_e/4" = FRiSO_univar2wass(X, Y, 0, Inf, tauseq, eps = epsseq[3]),
                   "fastfrechet_e/8" = FRiSO_univar2wass(X, Y, 0, Inf, tauseq, eps = epsseq[4]),
                   times = 5,
                   unit = "relative")

# Print times - milliseconds - rounding to 2 decimal places:
M$time = round(M$time / 1e6, 2) * 1e6
M
```

## Optimization Accuracy Comparison

We compare the two methods on optimization accuracy for the variable selection
problem on the example data set, using relative objective function ratios. The
optimal $\widehat{\pmb{\lambda}}$ minimizes an objective function (see the
accompanying vignette `intro-fastfrechet`, and @tucker_variable_2023 and
@coulter_fast_2024), so relative optimization accuracy can be measured by which
solution attains a more minimal objective function. For the `fastfrechet` error
tolerance of $\varepsilon = 0.014$, both methods attain comparably accurate
solutions. As expected, the `fastfrechet` solutions are more accurate for
smaller values of the error tolerance parameter.
```{r}
# Set sequence of error tolerances:
epsseq = 0.014 / c(1, 2, 4, 8)

# Calculate lambdas with fastfrechet implementation:
L_fastfrechet = sapply(epsseq, function(e){
  
  FRiSO_univar2wass(X, Y, 0, Inf, tauseq, eps = e)
  
}, simplify = "array")

# Calculate lambdas with Tucker et al. (2023) implementation:
L_Tucker = FRiSO_Tucker(X0, Y, 0, 1000, tauseq)
```

```{r, include = FALSE}
# Calculate objective function values for fastfrechet implementation:
f_fastfrechet = matrix(apply(expand.grid(tauseq, epsseq), 1, function(x){
  
  t = which(tauseq == x[1])
  e = which(epsseq == x[2])
  Q = frechetreg_univar2wass(X, Y, lambda = L_fastfrechet[ , t, e], lower = 0)$Qhat
  mean((Y - Q)^2)
  
}), ncol = 4)

# Calculate objective function values for Tucker et al. (2023) implementation:
f_Tucker = sapply(1:length(tauseq), function(t){
  
  Q = frechetreg_univar2wass(X, Y, lambda = L_Tucker[ , t], lower = 0)$Qhat
  mean((Y - Q)^2)
  
})

# Save graph:
png("friso_accuracy_comparison.png", width = 6.5, height = 4, units = "in", res = 576)

par(mai = c(0.55, 0.75, 0.08, 0.08))
plot(x = c(), y = c(), xlim = c(0, max(tauseq)), ylim = c(0.9999, 1.0001),
     axes = FALSE, xlab = "", ylab = "")
polygon(x = par("usr")[c(1, 2, 2, 1)], y = par("usr")[c(3, 3, 4, 4)],
        col = "gray93", border = NA)
abline(h = axTicks(2), v = axTicks(1), col = "white")
abline(h = 1, lty = 2, col = "gray50")
matlines(tauseq, f_fastfrechet / f_Tucker, type = 'b', col = 1:4, lty = 1,
         lwd = 1.5, pch = 15:18)
axis(1, NULL, TRUE, FALSE, -1.1, cex.axis = 0.9)
axis(2, c(0.9999, 1, 1.0001),
     c(expression("1 \U2013 10"^-4), expression("1.0"), expression("1 + 10"^-4)), FALSE, -0.9, las = 1, cex.axis = 0.9)
mtext(expression(tau), 1, 1.4, cex = 1.2)
mtext(expression("f("*bold(lambda)*")"["fastfrechet"]*" / f("*bold(lambda)*")"[Tucker]), 2, 2.4, las = 0)
legend("topleft", col = 1:4, pch = 15:18, lwd = 1, bty = "n",
       legend = sapply(1:4, function(e) bquote(epsilon*" = "*.(epsseq[e]))))

dev.off()
```

![](friso_accuracy_comparison.png){width=100%}
